{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11282971,"sourceType":"datasetVersion","datasetId":7054344},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900,"modelId":1902}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:34:43.841793Z","iopub.execute_input":"2025-04-05T09:34:43.842206Z","iopub.status.idle":"2025-04-05T09:35:01.178147Z","shell.execute_reply.started":"2025-04-05T09:34:43.842170Z","shell.execute_reply":"2025-04-05T09:35:01.177076Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nprint(\"GPU kullanılıyor mu?\", torch.cuda.is_available())\nprint(\"GPU ismi:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Yok\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:01.179732Z","iopub.execute_input":"2025-04-05T09:35:01.179979Z","iopub.status.idle":"2025-04-05T09:35:01.187921Z","shell.execute_reply.started":"2025-04-05T09:35:01.179959Z","shell.execute_reply":"2025-04-05T09:35:01.187147Z"}},"outputs":[{"name":"stdout","text":"GPU kullanılıyor mu? True\nGPU ismi: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:01.189821Z","iopub.execute_input":"2025-04-05T09:35:01.190056Z","iopub.status.idle":"2025-04-05T09:35:01.202487Z","shell.execute_reply.started":"2025-04-05T09:35:01.190037Z","shell.execute_reply":"2025-04-05T09:35:01.201698Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:01.203655Z","iopub.execute_input":"2025-04-05T09:35:01.203939Z","iopub.status.idle":"2025-04-05T09:35:01.728205Z","shell.execute_reply.started":"2025-04-05T09:35:01.203911Z","shell.execute_reply":"2025-04-05T09:35:01.727555Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:01.728875Z","iopub.execute_input":"2025-04-05T09:35:01.729070Z","iopub.status.idle":"2025-04-05T09:35:02.554551Z","shell.execute_reply.started":"2025-04-05T09:35:01.729053Z","shell.execute_reply":"2025-04-05T09:35:02.553853Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nThe token `colab-mistral` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `colab-mistral`\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"wandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:02.555581Z","iopub.execute_input":"2025-04-05T09:35:02.555816Z","iopub.status.idle":"2025-04-05T09:35:02.577299Z","shell.execute_reply.started":"2025-04-05T09:35:02.555796Z","shell.execute_reply":"2025-04-05T09:35:02.576676Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from datasets import load_dataset\n\nbase_model = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\ndataset_name = \"/kaggle/input/turkish-joke-dataset\"\nnew_model = \"mistral_7b_turkish_joke\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:02.578030Z","iopub.execute_input":"2025-04-05T09:35:02.578215Z","iopub.status.idle":"2025-04-05T09:35:02.582566Z","shell.execute_reply.started":"2025-04-05T09:35:02.578199Z","shell.execute_reply":"2025-04-05T09:35:02.581807Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Dataset'i yükle\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Her örneği instruction formatına çevir\ndef format_example(example):\n    return {\n        \"text\": f\"<s>[INST] Fıkra yazar mısın? [/INST]\\n{example['text']}</s>\"\n    }\n\nformatted_dataset = dataset.map(format_example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:02.583291Z","iopub.execute_input":"2025-04-05T09:35:02.583512Z","iopub.status.idle":"2025-04-05T09:35:02.710050Z","shell.execute_reply.started":"2025-04-05T09:35:02.583494Z","shell.execute_reply":"2025-04-05T09:35:02.709416Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        #load_in_4bit=True,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:02.712412Z","iopub.execute_input":"2025-04-05T09:35:02.712673Z","iopub.status.idle":"2025-04-05T09:35:08.964480Z","shell.execute_reply.started":"2025-04-05T09:35:02.712611Z","shell.execute_reply":"2025-04-05T09:35:08.963644Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"503a3f968db7407aa9024cb9f36206bc"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:08.965735Z","iopub.execute_input":"2025-04-05T09:35:08.965945Z","iopub.status.idle":"2025-04-05T09:35:09.068032Z","shell.execute_reply.started":"2025-04-05T09:35:08.965927Z","shell.execute_reply":"2025-04-05T09:35:09.067300Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(True, True)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:09.068858Z","iopub.execute_input":"2025-04-05T09:35:09.069165Z","iopub.status.idle":"2025-04-05T09:35:10.358805Z","shell.execute_reply.started":"2025-04-05T09:35:09.069135Z","shell.execute_reply":"2025-04-05T09:35:10.358138Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    save_steps=50,\n    logging_steps=25,\n    learning_rate=1e-4,  # veya 5e-5\n    weight_decay=0.01,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.1,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:10.359671Z","iopub.execute_input":"2025-04-05T09:35:10.359927Z","iopub.status.idle":"2025-04-05T09:35:10.388294Z","shell.execute_reply.started":"2025-04-05T09:35:10.359897Z","shell.execute_reply":"2025-04-05T09:35:10.387589Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def tokenize(example):\n    tokens = tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\n\ntokenized_dataset = dataset.map(tokenize)\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:10.388985Z","iopub.execute_input":"2025-04-05T09:35:10.389200Z","iopub.status.idle":"2025-04-05T09:35:10.428211Z","shell.execute_reply.started":"2025-04-05T09:35:10.389181Z","shell.execute_reply":"2025-04-05T09:35:10.427578Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    peft_config=peft_config,\n    args=training_arguments\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:10.428916Z","iopub.execute_input":"2025-04-05T09:35:10.429189Z","iopub.status.idle":"2025-04-05T09:35:10.551368Z","shell.execute_reply.started":"2025-04-05T09:35:10.429161Z","shell.execute_reply":"2025-04-05T09:35:10.550700Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:35:10.552159Z","iopub.execute_input":"2025-04-05T09:35:10.552374Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='471' max='864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [471/864 6:44:47 < 5:39:12, 0.02 it/s, Epoch 1.63/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.245600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.068900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.962300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.916200</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.856200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.851400</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.811100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.772300</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.844400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.782900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.815000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.683500</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.688000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.656600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.659900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.665500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.635900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.653100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Model'i kaydetme\n\ntrainer.model.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True\n!zip -r mistral_7b_turkish_joke.zip mistral_7b_turkish_joke","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model'i yayinlama\n'''\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nfrom transformers import pipeline\nfrom transformers import pipeline, logging\n\n# Gürültüyü azalt\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Fıkra yazar mısın?\"\n\n# Pipeline\npipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=200,\n    temperature=0.3,\n    top_p=0.9,\n    do_sample=True,\n    repetition_penalty=1.5\n)\n\n# Formatlı prompt ile çıktı al\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\\n\")\n\n# Sadece cevabı alalım, başlıkları filtreleyelim\ngenerated_text = result[0]['generated_text']\n\n# [INST] içeriğini ve </s> sonrası fazlalıkları kes\nif \"[/INST]\" in generated_text:\n    generated_text = generated_text.split(\"[/INST]\")[-1].strip()\nif \"</s>\" in generated_text:\n    generated_text = generated_text.split(\"</s>\")[0].strip()\n\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}